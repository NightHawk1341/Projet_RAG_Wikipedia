{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpdy2Hm9p02b"
      },
      "source": [
        "# TP : Génération augmentée de récupération (RAG) et agents\n",
        "\n",
        "Dans ce projet, le but va être de construire un système de RAG complet, à partir d'outils et de modèles libres existants.\n",
        "\n",
        "## Installation et importation des bibliothèques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On installe les bibliothèques nécessaires : LangChain pour l'orchestration et FAISS pour la base de données vectorielle, ainsi que le transformers et gdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U langchain-community\n",
        "!pip install faiss-gpu-cu12\n",
        "!pip install datasets\n",
        "!pip install langchain-text-splitters\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importation des bibliothèques\n",
        "\n",
        "Configuration de l'environnement et suppression des logs verbeux pour garder la sortie propre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "from langchain_community.docstore.document import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from smolagents import InferenceClientModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collection de documents (Wikipedia)\n",
        "\n",
        "On charge le jeu de données Wikipedia, crée des objets Document avec les métadonnées de la source, puis on les découpe en morceaux pour l'indexation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Chargement du jeu de données (10 premiers articles pour l'exemple de création)\n",
        "ds = load_dataset(\"Sketched33/Cities_Wikipedia_Information\", split=\"train[:10]\")\n",
        "\n",
        "# 2. Création des documents avec métadonnées\n",
        "source_docs = []\n",
        "for doc in ds:\n",
        "    source_docs.append(Document(\n",
        "        page_content=doc[\"wikipedia_content\"],\n",
        "        metadata={\"source\": \"Wikipedia - \" + doc[\"city_name\"]}\n",
        "    ))\n",
        "\n",
        "# 3. Configuration du découpeur (Splitter)\n",
        "embedding_model_name = \"thenlper/gte-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "    tokenizer,\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        "    add_start_index=True,\n",
        "    strip_whitespace=True,\n",
        ")\n",
        "\n",
        "# 4. Découpage des documents\n",
        "docs_processed = []\n",
        "docs_added = set()\n",
        "for doc in tqdm(source_docs, desc=\"Découpage des documents\"):\n",
        "    new_docs = text_splitter.split_documents([doc])\n",
        "    for new_doc in new_docs:\n",
        "        if new_doc.page_content not in docs_added:\n",
        "            docs_added.add(new_doc.page_content)\n",
        "            docs_processed.append(new_doc)\n",
        "\n",
        "print(f'\\nTotal : {len(docs_processed)} sous-documents créés.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Indexation et Chargement de la Base Vectorielle\n",
        "\n",
        "Pour gagner du temps et avoir une base de connaissances plus large, on télécharge ensuite un index pré-calculé de 3000 articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chargement du modèle d'embeddings\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# Création de l'index pour nos 10 documents (Démonstration)\n",
        "vectordb_demo = FAISS.from_documents(docs_processed, embedding=embedding_model, distance_strategy='cosine')\n",
        "print(\"Index de démonstration créé avec succès.\")\n",
        "\n",
        "# --- CHARGEMENT DE LA BASE DE DONNÉES COMPLÈTE (3000 articles) ---\n",
        "# On utilise une base plus large pour le système final\n",
        "!gdown --id 1a2WziQTnghzMuGfd5oHKynMSMjUC3kBJ -O index.zip\n",
        "!mkdir -p faiss_index\n",
        "!unzip -o index -d faiss_index\n",
        "\n",
        "vectordb = FAISS.load_local(\n",
        "    \"faiss_index\",\n",
        "    embedding_model, \n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "print(\"Base de données chargée avec succès !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration du LLM\n",
        "\n",
        "On va utiliser le modèle Qwen/Qwen2.5-72B-Instruct via l'API d'inférence Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# REMARQUE : Il faut rapmlacer \"hf_XXX\" par propre token Hugging Face\n",
        "api_token = \"hf_XXXXXXXXXXXXXXXXXX\"\n",
        "\n",
        "# Initialisation du modèle\n",
        "llm_engine = InferenceClientModel(\n",
        "    model_id=\"Qwen/Qwen2.5-72B-Instruct\", \n",
        "    token=api_token\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Système Complet (RAG Loop)\n",
        "\n",
        "On crée la boucle principale. L'utilisateur pose une question, le système cherche les 5 documents les plus pertinents dans la base FAISS, et le LLM génère une réponse basée sur ces sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prompt template pour le RAG\n",
        "rag_prompt = \"\"\"\n",
        "You are un AI assistant. Answer the user's questions using nothing but the following documents.\n",
        "If the answer can't be found in the documents, say \"I don't know\".\n",
        "For every answer you should provide a source file from which you got the information with the link to the coresponding wikipedia page.\n",
        "\n",
        "CONTEXT :\n",
        "Document 1 (Source: {src1}) : {doc1}\n",
        "Document 2 (Source: {src2}) : {doc2}\n",
        "Document 3 (Source: {src3}) : {doc3}\n",
        "Document 4 (Source: {src4}) : {doc4}\n",
        "Document 5 (Source: {src5}) : {doc5}\n",
        "\n",
        "User's question: {query}\n",
        "\n",
        "Answer :\n",
        "\"\"\"\n",
        "\n",
        "print(\"Assistant: Hi! How can I help you? (Type 'Exit' to quit)\")\n",
        "user_query = input('User: ')\n",
        "\n",
        "messages = []\n",
        "\n",
        "while user_query != \"Exit\":\n",
        "    # 1. Recherche (Retrieval)\n",
        "    sorted_docs = vectordb.similarity_search(user_query, k=5)\n",
        "    \n",
        "    # Préparation des variables pour le prompt\n",
        "    context_vars = {\n",
        "        \"query\": user_query,\n",
        "        \"doc1\": sorted_docs[0].page_content, \"src1\": sorted_docs[0].metadata['source'],\n",
        "        \"doc2\": sorted_docs[1].page_content, \"src2\": sorted_docs[1].metadata['source'],\n",
        "        \"doc3\": sorted_docs[2].page_content, \"src3\": sorted_docs[2].metadata['source'],\n",
        "        \"doc4\": sorted_docs[3].page_content, \"src4\": sorted_docs[3].metadata['source'],\n",
        "        \"doc5\": sorted_docs[4].page_content, \"src5\": sorted_docs[4].metadata['source'],\n",
        "    }\n",
        "    \n",
        "    # 2. Construction du prompt\n",
        "    full_prompt = rag_prompt.format(**context_vars)\n",
        "    \n",
        "    # 3. Génération (Generation)\n",
        "    # On garde un historique basique pour l'affichage, mais ici le RAG est \"stateless\" à chaque tour pour simplifier\n",
        "    messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
        "    answer = llm_engine(messages).content\n",
        "    \n",
        "    print(f'\\nAssistant: {answer}\\n')\n",
        "    \n",
        "    # Tour suivant\n",
        "    user_query = input('User: ')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
